{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html\n",
    "2. \"Uncertainty in deep learning\" Yarin Gal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib tk\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ..bayesian.callbacks import ModelTest\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad\n",
    "from keras.models import Sequential, Model\n",
    "from keras.models import Input\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n",
    "from keras.regularizers import l2\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.utils import to_categorical\n",
    "import itertools\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend as K\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = np.array([\n",
    "    [-1, -1],\n",
    "    [-0.7, -0.5],\n",
    "    [-0.5, -0.1],\n",
    "    [-0.2, 0.5], \n",
    "    [0, 0.8],\n",
    "    [0.2, 0.5],\n",
    "    [0.5, 0.2],\n",
    "    [0.7, 0.0],\n",
    "    [1, 0.3],\n",
    "    [1.2, 0.5],\n",
    "    [1.5, 0.8],\n",
    "    [3.5, 0.5],\n",
    "    [4.0, 0.35],\n",
    "    [4.5, 0.2],\n",
    "    [5.0, 0.05],\n",
    "    [5.5, -0.1]\n",
    "])\n",
    "x = xy[:, 0] *2 \n",
    "y = xy[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = np.array([\n",
    "    [-1, -1],\n",
    "    [-0.7, -0.5],\n",
    "    [-0.5, -0.1],\n",
    "    [-0.2, 0.5], \n",
    "    [0, 0.8],\n",
    "    [0.2, 0.5],\n",
    "    [0.5, 0.2],\n",
    "    [0.7, 0.0],\n",
    "    [1, 0.3],\n",
    "    [1.2, 0.5],\n",
    "    [1.5, 0.8],\n",
    "    [3.5, -0.5],\n",
    "    [4.0, 0.35],\n",
    "    [4.5, -0.2],\n",
    "    [5.0, 0.05],\n",
    "    [5.5, -0.5]\n",
    "])\n",
    "x = xy[:, 0] *2 \n",
    "y = xy[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.linspace(-10, 10, 300)\n",
    "# y = (1 - 2.5*np.power(x, 2)) * np.exp(-np.power(x, 2)) + np.random.normal(0, 0.05, size=len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_f(x):\n",
    "    v = np.zeros(len(x))\n",
    "    v[x < 2.5] = 0.1\n",
    "    v[x>=2.5]= x[x>2.5]*1.4\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_f(x):\n",
    "    v = np.copy(x)\n",
    "    v[v<0] = 1\n",
    "    v = np.log(v) * 0.08\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = np.concatenate([np.linspace(-10, -1.1, 100), np.linspace(1.1, 10, 100)])#, np.array([0, -0.1, 0.1])])\n",
    "#x = np.linspace(-10, 10, 200)\n",
    "x = np.random.uniform(-10, 10, 30)\n",
    "def f(x):\n",
    "    return (1 - 2.5*np.power(0.6*x, 2)) * np.exp(-np.power(0.6*x, 2))\n",
    "\n",
    "\n",
    "# def f(x):\n",
    "#     return np.exp(-np.sqrt(np.abs(x))) * np.cos(x)\n",
    "\n",
    "\n",
    "y = f(x) + np.random.normal(0, 0.1, size=len(x)) * var_f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'bo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = x.reshape(-1, 1)\n",
    "y_train = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_dropout = 0.05\n",
    "weight_decay = 0.001\n",
    "batch_size=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #regression\n",
    "# mean_y_train = np.mean(y_train)\n",
    "# std_y_train = np.std(y_train)\n",
    "# y_train = (y_train - mean_y_train) / std_y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayesian import BayesianDropoutModel, bayesian_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model:\n",
    "print('Build model...')\n",
    "model = BayesianDropoutModel()\n",
    "# model.add(Dropout(p_dropout, \n",
    "#                   input_shape=(X_train.shape[1], ),\n",
    "#                  ))\n",
    "model.add(Dense(25,\n",
    "                input_shape=(X_train.shape[1], ),\n",
    "                activation='relu',\n",
    "                kernel_regularizer=l2(weight_decay),\n",
    "                bias_regularizer=l2(weight_decay)))\n",
    "model.add(Dropout(p_dropout))\n",
    "model.add(Dense(25,\n",
    "                activation='sigmoid',\n",
    "                kernel_regularizer=l2(weight_decay),\n",
    "                bias_regularizer=l2(weight_decay)))\n",
    "model.add(Dropout(p_dropout))\n",
    "model.add(Dense(y_train.shape[1] if len(y_train.shape) > 1 else 1,\n",
    "                activation='linear',\n",
    "                kernel_regularizer=l2(weight_decay), bias_regularizer=l2(weight_decay)))\n",
    "\n",
    "optimiser = 'adam'\n",
    "model.compile(loss='mean_squared_error', optimizer=optimiser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model:\n",
    "print('Build model...')\n",
    "\n",
    "# model.add(Dropout(p_dropout, \n",
    "#                   input_shape=(X_train.shape[1], ),\n",
    "#                  ))\n",
    "inp = Input(shape=((X_train.shape[1], )))\n",
    "layer = Dense(25,\n",
    "           activation='relu',\n",
    "           kernel_regularizer=l2(weight_decay),\n",
    "           bias_regularizer=l2(weight_decay))(inp)\n",
    "layer = Dropout(p_dropout)(layer)\n",
    "layer = Dense(25,\n",
    "                activation='sigmoid',\n",
    "                kernel_regularizer=l2(weight_decay),\n",
    "                bias_regularizer=l2(weight_decay))(layer)\n",
    "layer = Dropout(p_dropout)(layer)\n",
    "output = Dense(y_train.shape[1] if len(y_train.shape) > 1 else 1,\n",
    "                activation='linear',\n",
    "                kernel_regularizer=l2(weight_decay), bias_regularizer=l2(weight_decay))(layer)\n",
    "\n",
    "model = BayesianDropoutModel([inp], [output])\n",
    "\n",
    "optimiser = SGD(lr=0.01, decay=1e-5)\n",
    "model.compile(loss='mean_squared_error', optimizer=optimiser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in model.layers:\n",
    "    print(type(l), l.input_shape, l.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Potentially load weights\n",
    "# model.load_weights(\"path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def standardize_X(X):\n",
    "#     if type(X) == list:\n",
    "#         return X\n",
    "#     else:\n",
    "#         return [X]\n",
    "    \n",
    "class DrawCallback(Callback):\n",
    "    def __init__(self, x, y, T, fig, ax):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.T = T\n",
    "        self.ax = ax\n",
    "        self.fig = fig\n",
    "        #self.mean_y_train = mean_y_train\n",
    "        #self.std_y_train = std_y_train\n",
    "        \n",
    "        #self.y = f(x)\n",
    "    \n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        #xx = np.linspace(x.min()-0.5, x.max()+0.5, 100).reshape(-1, 1)\n",
    "        #T = 100\n",
    "        prob = np.array([self.model.predict_stochastic(self.x, batch_size=500, verbose=0)\n",
    "                         for _ in xrange(self.T)])\n",
    "        prob_mean = np.mean(prob, 0) #* self.std_y_train + self.mean_y_train\n",
    "        prob_std = bayesian_std(prob, l=1, p_dropout=p_dropout, weight_decay=weight_decay, N=len(X_train))\n",
    "        \n",
    "        #fig.clf()\n",
    "        ax.cla()\n",
    "        #ax.plot(self.x, self.y, 'b-')\n",
    "        ax.plot(x, y, 'bo')\n",
    "        ax.plot(self.x, prob_mean, 'r--')\n",
    "        for n_std, alpha in itertools.izip([0.5, 1, 1.5], [0.3, 0.2, 0.1]):\n",
    "            ax.fill_between(self.x.ravel(),\n",
    "                     (prob_mean-n_std * prob_std).ravel(), \n",
    "                     (prob_mean+n_std * prob_std).ravel(), alpha=alpha, color='red')\n",
    "        ax.set_xlim(self.x.min()-0.5, self.x.max()+0.5)\n",
    "        ax.set_ylim(self.y.min()-0.5, self.y.max()+0.5)\n",
    "        fig.canvas.draw()\n",
    "        print('draw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "print(\"Train...\")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.show()\n",
    "draw = DrawCallback(x=np.linspace(x.min()-0.5, x.max()+0.5, 100).reshape(-1, 1), \n",
    "                   y=y_train,\n",
    "                   #mean_y_train=mean_y_train,\n",
    "                   #std_y_train=std_y_train,\n",
    "                   T=150,\n",
    "                   ax = ax,\n",
    "                   fig=fig)\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=10000, \n",
    "          callbacks=[draw], verbose=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayesian_mean_squared_error(y_true, pred_var):\n",
    "    log_var = pred_var[:, 1]\n",
    "    pred = pred_var[:, 0]\n",
    "    se = K.pow(K.transpose(y_true) - pred, 2) * K.exp(-log_var) + log_var\n",
    "    return K.mean(se)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model:\n",
    "print('Build model...')\n",
    "\n",
    "# model.add(Dropout(p_dropout, \n",
    "#                   input_shape=(X_train.shape[1], ),\n",
    "#                  ))\n",
    "inp = Input(shape=((X_train.shape[1], )))\n",
    "layer = Dense(25,\n",
    "           activation='relu',\n",
    "           kernel_regularizer=l2(weight_decay),\n",
    "           bias_regularizer=l2(weight_decay))(inp)\n",
    "layer = Dropout(p_dropout)(layer)\n",
    "layer = Dense(25,\n",
    "                activation='relu',\n",
    "                kernel_regularizer=l2(weight_decay),\n",
    "                bias_regularizer=l2(weight_decay))(layer)\n",
    "layer = Dropout(p_dropout)(layer)\n",
    "output = Dense(1,\n",
    "                activation='linear',\n",
    "                kernel_regularizer=l2(weight_decay), bias_regularizer=l2(weight_decay))(layer)\n",
    "\n",
    "var = Dense(1, activation='linear',\n",
    "            kernel_regularizer=l2(weight_decay), bias_regularizer=l2(weight_decay))(layer)\n",
    "\n",
    "final_output = Concatenate()([output, var])\n",
    "model = BayesianDropoutModel([inp], [final_output])\n",
    "\n",
    "optimiser = 'adam'\n",
    "model.compile(loss=bayesian_mean_squared_error, optimizer=optimiser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def standardize_X(X):\n",
    "#     if type(X) == list:\n",
    "#         return X\n",
    "#     else:\n",
    "#         return [X]\n",
    "    \n",
    "class DrawCallback2(Callback):\n",
    "    def __init__(self, x, y, T, fig, ax):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.T = T\n",
    "        self.ax = ax\n",
    "        self.fig = fig\n",
    "        #self.mean_y_train = mean_y_train\n",
    "        #self.std_y_train = std_y_train\n",
    "        \n",
    "        #self.y = f(x)\n",
    "    \n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        #xx = np.linspace(x.min()-0.5, x.max()+0.5, 100).reshape(-1, 1)\n",
    "        #T = 100\n",
    "        pred = np.array([self.model.predict_stochastic(self.x, batch_size=500, verbose=0)\n",
    "                         for _ in xrange(self.T)])\n",
    "        y_pred = pred[:, :, 0]\n",
    "        #print(y_pred.shape)\n",
    "        prob_mean = np.mean(y_pred, 0) #* self.std_y_train + self.mean_y_train\n",
    "        epistemic_prob_std = bayesian_std(y_pred, l=1, p_dropout=p_dropout, weight_decay=weight_decay, N=len(X_train))\n",
    "        \n",
    "        aleatoric_log_var = np.mean(pred[:, :, 1], 0)#self.model.predict(self.x)[:, 1]\n",
    "        aleatoric_var = np.exp(aleatoric_log_var)\n",
    "        aleatoric_std = np.sqrt(aleatoric_var)\n",
    "        \n",
    "        #fig.clf()\n",
    "        ax.cla()\n",
    "        ax.plot(self.x, f(self.x), 'b-')\n",
    "        ax.plot(x, y, 'bo')\n",
    "        ax.plot(self.x, prob_mean, 'r--')\n",
    "        \n",
    "        for n_std, alpha in itertools.izip([0.5, 1, 1.5], [0.3, 0.2, 0.1]):\n",
    "            ax.fill_between(self.x.ravel(),\n",
    "                     (prob_mean-n_std * epistemic_prob_std).ravel(), \n",
    "                     (prob_mean+n_std * epistemic_prob_std).ravel(), alpha=alpha, color='red')\n",
    "            \n",
    "            ax.fill_between(self.x.ravel(),\n",
    "                     (prob_mean-n_std * aleatoric_std).ravel(), \n",
    "                     (prob_mean+n_std * aleatoric_std).ravel(), alpha=alpha, color='green')\n",
    "            \n",
    "        ax.set_xlim(self.x.min()-0.5, self.x.max()+0.5)\n",
    "        ax.set_ylim(y_pred.min(), y_pred.max())\n",
    "        ax.set_ylim(self.y.min()-0.5, self.y.max()+0.5)\n",
    "        fig.canvas.draw()\n",
    "        #print('draw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "print(\"Train...\")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.show()\n",
    "draw = DrawCallback2(x=np.linspace(x.min()-0.5, x.max()+100.5, 200).reshape(-1, 1), \n",
    "                   y=y_train,\n",
    "                   #mean_y_train=mean_y_train,\n",
    "                   #std_y_train=std_y_train,\n",
    "                   T=50,\n",
    "                   ax = ax,\n",
    "                   fig=fig)\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=5000, \n",
    "          callbacks=[draw], verbose=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.array([model.predict_stochastic(X_train, batch_size=500, verbose=0)\n",
    "                         for _ in xrange(50)])\n",
    "aleatoric_log_var = np.mean(pred[:, :, 1], 0)#self.model.predict(self.x)[:, 1]\n",
    "aleatoric_var = np.exp(aleatoric_log_var)\n",
    "aleatoric_std = np.sqrt(aleatoric_var)\n",
    "\n",
    "aleatoric_std_2 = np.sqrt(np.exp(model.predict(X_train)[:, 1]))\n",
    "\n",
    "plt.plot(x, aleatoric_std, 'ro')\n",
    "plt.plot(x, aleatoric_std_2, 'bo')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
