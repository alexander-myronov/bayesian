{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib tk\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from bayesian.callbacks import ModelTest\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad\n",
    "from keras.models import Sequential, Input\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n",
    "from keras.regularizers import l2\n",
    "import keras.metrics\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_dropout = 0.5\n",
    "weight_decay = 0.01\n",
    "batch_size=400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=np.random.randint(0, len(y_train))\n",
    "# plt.imshow(X_train[i], cmap=plt.cm.get_cmap('Greys'))\n",
    "# plt.title(y_train[i])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(len(X_train), -1)\n",
    "X_test = X_test.reshape(len(X_test), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification\n",
    "mean_y_train = 0\n",
    "std_y_train = 1\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_train.min(), X_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayesian.bayesian_dropout_model import BayesianDropoutModel\n",
    "from bayesian.metrics import variation_ratio\n",
    "from bayesian.objectives import bayesian_categorical_crossentropy_original, noise_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(theano.printing.debugprint(model.outputs[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model:\n",
    "print('Build model...')\n",
    "\n",
    "inp = Input(shape=(X_train.shape[1],))\n",
    "\n",
    "xx = Dense(200,\n",
    "            activation='relu',\n",
    "            kernel_regularizer=l2(weight_decay),\n",
    "            bias_regularizer=l2(weight_decay))(inp)\n",
    "\n",
    "xx = Dropout(p_dropout)(xx)\n",
    "\n",
    "xx = Dense(100,\n",
    "            activation='relu',\n",
    "            kernel_regularizer=l2(weight_decay),\n",
    "            bias_regularizer=l2(weight_decay))(xx)\n",
    "xx = Dropout(p_dropout)(xx)\n",
    "logits = Dense(y_train.shape[1], \n",
    "               kernel_regularizer=l2(weight_decay), bias_regularizer=l2(weight_decay))(xx)\n",
    "output = Activation('softmax')(logits)\n",
    "            \n",
    "variance_pre = Dense(1)(xx)\n",
    "variance = Activation('softplus', name='variance')(variance_pre)\n",
    "logits_variance = Concatenate(name='logits_variance')([logits, variance])\n",
    "softmax_output = Activation('softmax', name='softmax_output')(logits)\n",
    "    \n",
    "model = BayesianDropoutModel([inp], [softmax_output, logits_variance])\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={\n",
    "    'logits_variance': bayesian_categorical_crossentropy_original(100, 10),\n",
    "    'softmax_output': 'categorical_crossentropy'\n",
    "    },\n",
    "    metrics={'softmax_output': keras.metrics.categorical_accuracy},\n",
    "    loss_weights={'logits_variance': 1, \n",
    "                  'softmax_output': 0\n",
    "                }\n",
    ")\n",
    "\n",
    "# model = BayesianDropoutModel([inp], [logits_variance])\n",
    "#model.compile(optimizer='adam', loss=bayesian_categorical_crossentropy_original(100, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "print(\"Train...\")\n",
    "\n",
    "# Theano\n",
    "# modeltest_1 = ModelTest(X_train[:100], \n",
    "#                         mean_y_train + std_y_train * np.atleast_2d(y_train[:100]), \n",
    "#                         test_every_X_epochs=1, verbose=0, loss='categorical', \n",
    "#                         mean_y_train=mean_y_train, std_y_train=std_y_train)\n",
    "# modeltest_2 = ModelTest(X_test, \n",
    "#                         np.atleast_2d(y_test),\n",
    "#                         test_every_X_epochs=1, \n",
    "#                         verbose=0, loss='categorical', \n",
    "#                         mean_y_train=mean_y_train, std_y_train=std_y_train)\n",
    "model.fit([X_train],\n",
    "          [y_train]*2,\n",
    "          batch_size=batch_size,\n",
    "          epochs=100, \n",
    "          #callbacks=[modeltest_1, modeltest_2],\n",
    "          verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_classification(y_true, y_pred):\n",
    "    return accuracy_score(y_true.argmax(-1), y_pred.argmax(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_regression(y_true, y_pred):\n",
    "    raise Exception('not correct')\n",
    "    return (np.mean(((mean_y_train + std_y_train * np.atleast_2d(y_true).T)\n",
    "               - (y_pred + std_y_train * standard_prob))**2, 0)**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "# Dropout approximation for training data:\n",
    "standard_prob = model.predict(X_train, batch_size=500, verbose=0)[0]\n",
    "print(score_classification(y_train, standard_prob))\n",
    "\n",
    "# MC dropout for test data:\n",
    "T = 50\n",
    "prob = np.array([model.predict_stochastic(X_test, batch_size=500, verbose=0)\n",
    "                 for _ in range(T)])\n",
    "prob_mean = np.mean(prob, 0)\n",
    "print(score_classification(y_test, prob_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model - the trained classifier(C classes) \n",
    "#where the last layer applies softmax\n",
    "# X_data - a list of input data(size N)\n",
    "# T - the number of monte carlo simulations to run\n",
    "def montecarlo_prediction(model, X_data, T):\n",
    "# shape: (T, N, C)\n",
    "    predictions = np.array([model.predict_stochastic(X_data, batch_size=750, verbose=True) for _ in range(T)])\n",
    "    #print(predictions.min())\n",
    "    \n",
    "    # shape: (N, C)\n",
    "    prediction_probabilities = np.mean(predictions, axis=0)\n",
    "    \n",
    "    #print(prediction_probabilities.shape)\n",
    "    # shape: (N)\n",
    "    prediction_variances = np.apply_along_axis(predictive_entropy, axis=1, arr=prediction_probabilities)\n",
    "    return (prediction_probabilities, prediction_variances)\n",
    "\n",
    "# prob - prediction probability for each class(C). Shape: (N, C)\n",
    "# returns - Shape: (N)\n",
    "def predictive_entropy(prob):\n",
    "    #print(prob.shape)\n",
    "    return -1 * np.sum(np.log(prob) * prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs, epistemic = montecarlo_prediction(model, X_test, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aleatoric = np.sqrt(model.predict(X_test, batch_size=500, verbose=0)[1][:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_pred = prob.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = mode(prob_pred, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(prob_mean.argmax(-1) == m.mode).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 25\n",
    "#p=prob_std / prob_std.sum()\n",
    "#vr = variation_ratio(prob)\n",
    "p = epistemic / epistemic.mean() # \n",
    "p = aleatoric / aleatoric.mean()\n",
    "p = p / p.sum()\n",
    "\n",
    "fig, axs = plt.subplots(ncols=int(k / 5), nrows=5)\n",
    "axs = axs.ravel()\n",
    "for i, i_test in enumerate(np.random.choice(len(y_test), k, p=p)):\n",
    "    img = X_test[i_test].reshape(28, 28) * 255\n",
    "    axs[i].imshow(img, cmap=plt.cm.get_cmap('Greys'))\n",
    "    axs[i].set_title('%d - %d, ale=%.3f, epi=%.3f' % (y_test[i_test].argmax(), \n",
    "                                               prob_mean[i_test].argmax(),\n",
    "                                               aleatoric[i_test],\n",
    "                                               epistemic[i_test]))\n",
    "fig.set_size_inches(15, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(111)\n",
    "for yy in range(0, 10):\n",
    "    index = y_test[:, yy].astype(bool)\n",
    "    plt.scatter(epistemic[index], aleatoric[index], marker='$%d$' % yy, alpha=0.4, s=100)\n",
    "fig.set_size_inches(16, 12)\n",
    "plt.xlabel('epistemic')\n",
    "plt.ylabel('aleatoric')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(epistemic/epistemic.mean())\n",
    "plt.hist(aleatoric/aleatoric.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 10\n",
    "x0 = X_train[i]\n",
    "y0 = y_train[i]\n",
    "y_opt = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x0.reshape(28, 28), cmap='Greys')\n",
    "plt.title('actual=%d, predicted=%d' % (y0.argmax(), model.predict(x0.reshape(1, -1))[0].argmax()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_f(delta_x):\n",
    "    return 1 - model.predict((x0+delta_x).reshape(1, -1))[0, y_opt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_t = K.placeholder((None, len(x0)), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(delta_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = 1 - model.outputs[0][0, y_opt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = K.function([delta_t], [cost], \n",
    "               givens={\n",
    "                   K.learning_phase(): np.uint8(0),\n",
    "                   model.input: delta_t + x0.astype('float32')\n",
    "               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = K.gradients(cost, model.input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_prime = K.function([delta_t], [grad], \n",
    "               givens={\n",
    "                   K.learning_phase(): np.uint8(0),\n",
    "                   model.input: delta_t + x0.astype('float32')\n",
    "               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cb(x):\n",
    "    print(min_f(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_f(delta):\n",
    "    return f([delta.reshape(1, -1)])[0]\n",
    "\n",
    "def min_f_prime(delta):\n",
    "    return f_prime([delta.reshape(1, -1)])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = scipy.optimize.fmin_tnc(min_f,\n",
    "                              x0=np.random.normal(0, 1, size=len(x0)),\n",
    "                              fprime=min_f_prime,\n",
    "                              bounds=[(-0.45, 0.45)] * len(x0),\n",
    "                              callback=cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(res[0].reshape(28, 28), cmap='Greys', vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model - the trained classifier(C classes) \n",
    "# where the last layer applies softmax\n",
    "# X_data - a list of input data(size N)\n",
    "# T - the number of monte carlo simulations to run\n",
    "def montecarlo_prediction_epistemic_aleatoric(model, X_data, T):\n",
    "# shape: (T, N, C)\n",
    "    predictions = np.array([model.predict_stochastic(X_data, batch_size=750, verbose=False) for _ in range(T)])\n",
    "    #print(predictions.shape)\n",
    "    \n",
    "    # shape: (N, C)\n",
    "    prediction_probabilities = np.mean(predictions, axis=0)\n",
    "    \n",
    "    #print(prediction_probabilities.shape)\n",
    "    # shape: (N)\n",
    "    prediction_variances = np.apply_along_axis(predictive_entropy, axis=1, arr=prediction_probabilities)\n",
    "    \n",
    "    var = model.predict(X_data, batch_size=500)[1][:, -1]\n",
    "    \n",
    "    \n",
    "    return (prediction_probabilities, prediction_variances, var)\n",
    "\n",
    "# prob - prediction probability for each class(C). Shape: (N, C)\n",
    "# returns - Shape: (N)\n",
    "def predictive_entropy(prob):\n",
    "    #print(prob.shape)\n",
    "    return -1 * np.sum(np.log(prob) * prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict((x0+res[0]).reshape(1, -1))[0].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob, epi, ale = montecarlo_prediction_epistemic_aleatoric(model, (x0+res[0]).reshape(1, -1), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob.argmax(), epi, ale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow((res[0] + x0).reshape(28, 28), cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_predict_stochastic = K.function([model.inputs[0]], [model.outputs[1]],\n",
    "                                                      givens={K.learning_phase(): np.uint8(1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "o1 = np.sqrt(np.array([model._predict_loop(_predict_stochastic,\n",
    "                                   [np.atleast_2d(x0)], batch_size=batch_size, verbose=False)[:, -1]\n",
    "               for _ in xrange(200)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o2 = np.sqrt(np.array([model._predict_loop(_predict_stochastic,\n",
    "                                   [np.atleast_2d(x0+res[0])], batch_size=batch_size, verbose=False)[:, -1]\n",
    "               for _ in xrange(200)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o1.mean(), o1.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o2.mean(), o2.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs0 = np.array([model.predict_stochastic(np.atleast_2d(x0))[0] for _ in xrange(100)])\n",
    "\n",
    "probs0.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs0 = np.array([model.predict_stochastic(np.atleast_2d(x0+res[0]))[0] for _ in xrange(100)])\n",
    "\n",
    "probs0.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
