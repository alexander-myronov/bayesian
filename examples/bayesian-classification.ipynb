{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib tk\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from callbacks import ModelTest\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad\n",
    "from keras.models import Sequential, Input\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n",
    "from keras.regularizers import l2\n",
    "import keras.metrics\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "p_dropout = 0.5\n",
    "weight_decay = 0.01\n",
    "batch_size=400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# i=np.random.randint(0, len(y_train))\n",
    "# plt.imshow(X_train[i], cmap=plt.cm.get_cmap('Greys'))\n",
    "# plt.title(y_train[i])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(len(X_train), -1)\n",
    "X_test = X_test.reshape(len(X_test), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#regression\n",
    "# mean_y_train = np.mean(y_train)\n",
    "# std_y_train = np.std(y_train)\n",
    "# y_train = (y_train - mean_y_train) / std_y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#classification\n",
    "mean_y_train = 0\n",
    "std_y_train = 1\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train.shape, X_train.min(), X_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from bayesian import BayesianDropoutModel, variation_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "theano_rng = MRG_RandomStreams(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# original loss from https://arxiv.org/pdf/1703.04977.pdf \n",
    "def bayesian_categorical_crossentropy_original(T, num_classes):\n",
    "    def bayesian_categorical_crossentropy_internal_original(true, pred_var):\n",
    "        # shape: (N,)\n",
    "        std = K.sqrt(pred_var[:, num_classes])\n",
    "        # shape: (N,)\n",
    "        variance = pred_var[:, num_classes:]\n",
    "        variance_depressor = K.exp(variance) - K.ones_like(variance)\n",
    "        # shape: (N, C)\n",
    "        pred = pred_var[:, 0:num_classes]\n",
    "        # shape: (N,)\n",
    "        undistorted_loss = K.categorical_crossentropy(pred, true, from_logits=True)\n",
    "        # shape: (T,)\n",
    "        iterable = K.variable(np.ones(T))\n",
    "        dist = theano_rng.normal(avg=0, std=K.tile(std, (pred.shape[1], 1)).T, size=pred.shape)\n",
    "        #distributions.Normal(loc=K.zeros_like(std), scale=std)\n",
    "        monte_carlo_results = K.map_fn(gaussian_categorical_crossentropy_original(true, pred, dist), iterable, name='monte_carlo_results')\n",
    "\n",
    "        variance_loss = K.mean(monte_carlo_results, axis=0)# * undistorted_loss\n",
    "        #return K.mean(theano_rng.normal(avg=0, std=1, size=(10,)))\n",
    "        # return undistorted_loss\n",
    "        return variance_loss# + undistorted_loss + variance_depressor\n",
    "  \n",
    "    return bayesian_categorical_crossentropy_internal_original\n",
    "\n",
    "# for a single monte carlo simulation, \n",
    "#   calculate categorical_crossentropy of \n",
    "#   predicted logit values plus gaussian \n",
    "#   noise vs true values.\n",
    "# true - true values. Shape: (N, C)\n",
    "# pred - predicted logit values. Shape: (N, C)\n",
    "# dist - normal distribution to sample from. Shape: (N, C)\n",
    "# undistorted_loss - the crossentropy loss without variance distortion. Shape: (N,)\n",
    "# num_classes - the number of classes. C\n",
    "# returns - total differences for all classes (N,)\n",
    "def gaussian_categorical_crossentropy_original(true, pred, dist):\n",
    "    def map_fn(i):\n",
    "        std_samples = dist#K.transpose(dist.sample(num_classes))\n",
    "        distorted_logits = pred + std_samples\n",
    "        #distorted_loss = K.categorical_crossentropy(distorted_logits, true, from_logits=True)\n",
    "        #return distorted_loss\n",
    "        #return K.categorical_crossentropy(true, pred, from_logits=True)\n",
    "        #return K.log(K.sum(K.exp(pred), axis=1))\n",
    "        distorted_loss = -K.sum(distorted_logits * true, axis=1) + K.log(K.sum(K.exp(distorted_logits), axis=1))\n",
    "        distorted_loss2 = K.categorical_crossentropy(distorted_logits, true, from_logits=True)\n",
    "        return distorted_loss - distorted_loss2\n",
    "        \n",
    "    return map_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# modified loss from https://medium.com/towards-data-science/building-a-bayesian-deep-learning-classifier-ece1845bc09\n",
    "# standard categorical cross entropy\n",
    "# N data points, C classes\n",
    "# true - true values. Shape: (N, C)\n",
    "# pred - predicted values. Shape: (N, C)\n",
    "# returns - loss (N)\n",
    "def categorical_cross_entropy(true, pred):\n",
    "    return np.sum(true * np.log(pred), axis=1)\n",
    "\n",
    "# Bayesian categorical cross entropy.\n",
    "# N data points, C classes, T monte carlo simulations\n",
    "# true - true values. Shape: (N, C)\n",
    "# pred_var - predicted logit values and variance. Shape: (N, C + 1)\n",
    "# returns - loss (N,)\n",
    "\n",
    "def bayesian_categorical_crossentropy_elu(T, num_classes):\n",
    "    def bayesian_categorical_crossentropy_internal_elu(true, pred_var):\n",
    "        # shape: (N,)\n",
    "        std = K.sqrt(pred_var[:, num_classes])\n",
    "        # shape: (N,)\n",
    "        variance = pred_var[:, num_classes]\n",
    "        variance_depressor = K.exp(variance) - K.ones_like(variance)\n",
    "        # shape: (N, C)\n",
    "        pred = pred_var[:, 0:num_classes]\n",
    "        # shape: (N,)\n",
    "        undistorted_loss = K.categorical_crossentropy(pred, true, from_logits=True)\n",
    "        # shape: (T,)\n",
    "        iterable = K.variable(np.ones(T))\n",
    "        dist = theano_rng.normal(avg=0, std=K.tile(std, (pred.shape[1], 1)).T, size=pred.shape)\n",
    "        monte_carlo_results = K.map_fn(gaussian_categorical_crossentropy(true, pred, dist, undistorted_loss, num_classes), iterable, name='monte_carlo_results')\n",
    "\n",
    "        variance_loss = K.mean(monte_carlo_results, axis=0) * undistorted_loss\n",
    "        return variance_loss + undistorted_loss + variance_depressor\n",
    "  \n",
    "    return bayesian_categorical_crossentropy_internal_elu\n",
    "\n",
    "# for a single monte carlo simulation, \n",
    "#   calculate categorical_crossentropy of \n",
    "#   predicted logit values plus gaussian \n",
    "#   noise vs true values.\n",
    "# true - true values. Shape: (N, C)\n",
    "# pred - predicted logit values. Shape: (N, C)\n",
    "# dist - normal distribution to sample from. Shape: (N, C)\n",
    "# undistorted_loss - the crossentropy loss without variance distortion. Shape: (N,)\n",
    "# num_classes - the number of classes. C\n",
    "# returns - total differences for all classes (N,)\n",
    "def gaussian_categorical_crossentropy(true, pred, dist, undistorted_loss, num_classes):\n",
    "    def map_fn(i):\n",
    "        std_samples = dist\n",
    "        distorted_loss = K.categorical_crossentropy(pred+dist, true, from_logits=True)\n",
    "        diff = undistorted_loss - distorted_loss\n",
    "        return -K.elu(diff)\n",
    "    return map_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# print(theano.printing.debugprint(model.outputs[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Build model:\n",
    "print('Build model...')\n",
    "\n",
    "inp = Input(shape=(X_train.shape[1],))\n",
    "\n",
    "xx = Dense(200,\n",
    "            activation='relu',\n",
    "            kernel_regularizer=l2(weight_decay),\n",
    "            bias_regularizer=l2(weight_decay))(inp)\n",
    "\n",
    "xx = Dropout(p_dropout)(xx)\n",
    "\n",
    "xx = Dense(100,\n",
    "            activation='relu',\n",
    "            kernel_regularizer=l2(weight_decay),\n",
    "            bias_regularizer=l2(weight_decay))(xx)\n",
    "xx = Dropout(p_dropout)(xx)\n",
    "logits = Dense(y_train.shape[1], \n",
    "               kernel_regularizer=l2(weight_decay), bias_regularizer=l2(weight_decay))(xx)\n",
    "output = Activation('softmax')(logits)\n",
    "            \n",
    "variance_pre = Dense(1)(xx)\n",
    "variance = Activation('softplus', name='variance')(variance_pre)\n",
    "logits_variance = Concatenate(name='logits_variance')([logits, variance])\n",
    "#softmax_output = Activation('softmax', name='softmax_output')(logits)\n",
    "    \n",
    "#model = BayesianDropoutModel([inp], [softmax_output, logits_variance])\n",
    "\n",
    "\n",
    "# model.compile(\n",
    "#     optimizer='adam',\n",
    "#     loss={\n",
    "#     'logits_variance': bayesian_categorical_crossentropy_original(100, 10),\n",
    "#      'softmax_output': 'categorical_crossentropy'\n",
    "#     },\n",
    "#     metrics={'softmax_output': keras.metrics.categorical_accuracy},\n",
    "#     loss_weights={'logits_variance': 1, \n",
    "#                   'softmax_output': 0\n",
    "#                 }\n",
    "# )\n",
    "\n",
    "model = BayesianDropoutModel([inp], [logits_variance])\n",
    "model.compile(optimizer='adam', loss=bayesian_categorical_crossentropy_original(100, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# for l in model.layers:\n",
    "#     print(l, l.input_shape, l.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "print(\"Train...\")\n",
    "\n",
    "# Theano\n",
    "# modeltest_1 = ModelTest(X_train[:100], \n",
    "#                         mean_y_train + std_y_train * np.atleast_2d(y_train[:100]), \n",
    "#                         test_every_X_epochs=1, verbose=0, loss='categorical', \n",
    "#                         mean_y_train=mean_y_train, std_y_train=std_y_train)\n",
    "# modeltest_2 = ModelTest(X_test, \n",
    "#                         np.atleast_2d(y_test),\n",
    "#                         test_every_X_epochs=1, \n",
    "#                         verbose=0, loss='categorical', \n",
    "#                         mean_y_train=mean_y_train, std_y_train=std_y_train)\n",
    "model.fit(X_train,\n",
    "          [y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=100, \n",
    "          #callbacks=[modeltest_1, modeltest_2],\n",
    "          verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import theano\n",
    "# theano.config.optimizer = 'fast_run'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def score_classification(y_true, y_pred):\n",
    "    return accuracy_score(y_true.argmax(-1), y_pred.argmax(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def score_regression(y_true, y_pred):\n",
    "    raise Exception('not correct')\n",
    "    return (np.mean(((mean_y_train + std_y_train * np.atleast_2d(y_true).T)\n",
    "               - (y_pred + std_y_train * standard_prob))**2, 0)**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "# Dropout approximation for training data:\n",
    "standard_prob = model.predict(X_train, batch_size=500, verbose=0)\n",
    "print(score_classification(y_train, standard_prob))\n",
    "\n",
    "# MC dropout for test data:\n",
    "T = 50\n",
    "prob = np.array([model.predict_stochastic(X_test, batch_size=500, verbose=0)\n",
    "                 for _ in xrange(T)])\n",
    "prob_mean = np.mean(prob, 0)\n",
    "print(score_classification(y_test, prob_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# model - the trained classifier(C classes) \n",
    "#where the last layer applies softmax\n",
    "# X_data - a list of input data(size N)\n",
    "# T - the number of monte carlo simulations to run\n",
    "def montecarlo_prediction(model, X_data, T):\n",
    "# shape: (T, N, C)\n",
    "    predictions = np.array([model.predict_stochastic(X_data, batch_size=750, verbose=True) for _ in range(T)])\n",
    "    #print(predictions.shape)\n",
    "    \n",
    "    # shape: (N, C)\n",
    "    prediction_probabilities = np.mean(predictions, axis=0)\n",
    "    \n",
    "    #print(prediction_probabilities.shape)\n",
    "    # shape: (N)\n",
    "    prediction_variances = np.apply_along_axis(predictive_entropy, axis=1, arr=prediction_probabilities)\n",
    "    return (prediction_probabilities, prediction_variances)\n",
    "\n",
    "# prob - prediction probability for each class(C). Shape: (N, C)\n",
    "# returns - Shape: (N)\n",
    "def predictive_entropy(prob):\n",
    "    #print(prob.shape)\n",
    "    return -1 * np.sum(np.log(prob) * prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "probs, epistemic = montecarlo_prediction(model, X_test, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "epistemic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "aleatoric.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "aleatoric = np.sqrt(model.predict(X_test, batch_size=500, verbose=0)[1][:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "prob_pred = prob.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "m = mode(prob_pred, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "m.mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "(prob_mean.argmax(-1) == m.mode).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def variation_ratio(y_pred):\n",
    "    prob_pred = prob.argmax(axis=-1)\n",
    "    m = mode(prob_pred, 0)\n",
    "    return (1 - (m.count / float(y_pred.shape[0]))).ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "variation_ratio(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# prob_std = np.std(prob, axis=0).ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# y_pred = mean_y_train + std_y_train * prob_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# ind = np.argsort(y_test)\n",
    "# plt.plot(y_test[ind], y_pred[ind], 'r+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "k = 25\n",
    "#p=prob_std / prob_std.sum()\n",
    "#vr = variation_ratio(prob)\n",
    "p = epistemic / epistemic.mean() # \n",
    "p = aleatoric / aleatoric.mean()\n",
    "p = p / p.sum()\n",
    "\n",
    "fig, axs = plt.subplots(ncols=(k / 5), nrows=5)\n",
    "axs = axs.ravel()\n",
    "for i, i_test in enumerate(np.random.choice(len(y_test), k, p=p)):\n",
    "    img = X_test[i_test].reshape(28, 28) * 255\n",
    "    axs[i].imshow(img, cmap=plt.cm.get_cmap('Greys'))\n",
    "    axs[i].set_title('%d - %d, ale=%.3f, epi=%.3f' % (y_test[i_test].argmax(), \n",
    "                                               prob_mean[i_test].argmax(),\n",
    "                                               aleatoric[i_test],\n",
    "                                               epistemic[i_test]))\n",
    "fig.set_size_inches(15, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(111)\n",
    "for yy in xrange(0, 10):\n",
    "    index = y_test[:, yy].astype(bool)\n",
    "    plt.scatter(epistemic[index], aleatoric[index], marker='$%d$' % yy, alpha=0.4, s=100)\n",
    "fig.set_size_inches(16, 12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.hist(epistemic/epistemic.mean())\n",
    "plt.hist(aleatoric/aleatoric.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "i = 10\n",
    "x0 = X_train[i]\n",
    "y0 = y_train[i]\n",
    "y_opt = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(x0.reshape(28, 28), cmap='Greys')\n",
    "plt.title('actual=%d, predicted=%d' % (y0.argmax(), model.predict(x0.reshape(1, -1))[0].argmax()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def min_f(delta_x):\n",
    "    return 1 - model.predict((x0+delta_x).reshape(1, -1))[0, y_opt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "delta_t = K.placeholder((None, len(x0)), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "type(delta_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cost = 1 - model.outputs[0][0, y_opt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "f = K.function([delta_t], [cost], \n",
    "               givens={\n",
    "                   K.learning_phase(): np.uint8(0),\n",
    "                   model.input: delta_t + x0.astype('float32')\n",
    "               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "grad = K.gradients(cost, model.input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "f_prime = K.function([delta_t], [grad], \n",
    "               givens={\n",
    "                   K.learning_phase(): np.uint8(0),\n",
    "                   model.input: delta_t + x0.astype('float32')\n",
    "               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import scipy.optimize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cb(x):\n",
    "    print(min_f(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def min_f(delta):\n",
    "    return f([delta.reshape(1, -1)])[0]\n",
    "\n",
    "def min_f_prime(delta):\n",
    "    return f_prime([delta.reshape(1, -1)])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "res = scipy.optimize.fmin_tnc(min_f,\n",
    "                              x0=np.random.normal(0, 1, size=len(x0)),\n",
    "                              fprime=min_f_prime,\n",
    "                              bounds=[(-0.45, 0.45)] * len(x0),\n",
    "                              callback=cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(res[0].reshape(28, 28), cmap='Greys', vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# model - the trained classifier(C classes) \n",
    "#where the last layer applies softmax\n",
    "# X_data - a list of input data(size N)\n",
    "# T - the number of monte carlo simulations to run\n",
    "def montecarlo_prediction_epistemic_aleatoric(model, X_data, T):\n",
    "# shape: (T, N, C)\n",
    "    predictions = np.array([model.predict_stochastic(X_data, batch_size=750, verbose=False) for _ in range(T)])\n",
    "    #print(predictions.shape)\n",
    "    \n",
    "    # shape: (N, C)\n",
    "    prediction_probabilities = np.mean(predictions, axis=0)\n",
    "    \n",
    "    #print(prediction_probabilities.shape)\n",
    "    # shape: (N)\n",
    "    prediction_variances = np.apply_along_axis(predictive_entropy, axis=1, arr=prediction_probabilities)\n",
    "    \n",
    "    var = model.predict(X_data, batch_size=500)[1][:, -1]\n",
    "    \n",
    "    \n",
    "    return (prediction_probabilities, prediction_variances, var)\n",
    "\n",
    "# prob - prediction probability for each class(C). Shape: (N, C)\n",
    "# returns - Shape: (N)\n",
    "def predictive_entropy(prob):\n",
    "    #print(prob.shape)\n",
    "    return -1 * np.sum(np.log(prob) * prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.predict((x0+res[0]).reshape(1, -1))[0].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "prob, epi, ale = montecarlo_prediction_epistemic_aleatoric(model, (x0+res[0]).reshape(1, -1), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "prob.argmax(), epi, ale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.imshow((res[0] + x0).reshape(28, 28), cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "_predict_stochastic = K.function([model.inputs[0]], [model.outputs[1]],\n",
    "                                                      givens={K.learning_phase(): np.uint8(1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "o1 = np.sqrt(np.array([model._predict_loop(_predict_stochastic,\n",
    "                                   [np.atleast_2d(x0)], batch_size=batch_size, verbose=False)[:, -1]\n",
    "               for _ in xrange(200)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "o2 = np.sqrt(np.array([model._predict_loop(_predict_stochastic,\n",
    "                                   [np.atleast_2d(x0+res[0])], batch_size=batch_size, verbose=False)[:, -1]\n",
    "               for _ in xrange(200)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "o1.mean(), o1.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "o2.mean(), o2.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "probs0 = np.array([model.predict_stochastic(np.atleast_2d(x0))[0] for _ in xrange(100)])\n",
    "\n",
    "probs0.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "probs0 = np.array([model.predict_stochastic(np.atleast_2d(x0+res[0]))[0] for _ in xrange(100)])\n",
    "\n",
    "probs0.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
